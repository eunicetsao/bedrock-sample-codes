{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install botocore-1.32.4-py3-none-any.whl\n",
    "!python3 -m pip install boto3-1.29.4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "module_path = \".\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    region='us-east-1',\n",
    "    runtime=False\n",
    ")\n",
    "\n",
    "boto3_bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    region='us-east-1',\n",
    "    runtime=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt_template.txt\", \"r\") as f:\n",
    "    prompt_string = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt_string, input_variables=[\"english_string\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(english_string=\"A friend in need is a friend indeed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body =  json.dumps({\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens_to_sample\": 100,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 1.,\n",
    "    \"stop_sequences\":[\"\\n\\nHuman:\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = 'anthropic.claude-v2:1' # change this to use a different version from the model provider\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "response = boto3_bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "print_ww(response_body.get('completion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body =  json.dumps({\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens_to_sample\": 100,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 1.,\n",
    "    \"stop_sequences\":[\"\\n\\nHuman:\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "gen_inputs = []\n",
    "for j in range(10):\n",
    "    body_obj = {\n",
    "        \"recordId\" : ''.join(random.choice('0123456789ABCDEF') for i in range(12)), \"modelInput\": {\n",
    "        \"prompt\" : prompt,\n",
    "        \"max_tokens_to_sample\" : 100,\n",
    "        \"temperature\" : 0.1,\n",
    "        \"top_k\" : 3,\n",
    "        \"top_p\" : 1.,\n",
    "        \"stop_sequences\" :[\"\\n\\nHuman:\"]\n",
    "    }}\n",
    "    gen_inputs.append(body_obj)\n",
    "\n",
    "with open('fake_data.jsonl', 'a') as outfile:\n",
    "    for response in gen_inputs:\n",
    "        json.dump(response, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference performance with 3000 samples with invoke_model api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import requests as req\n",
    "import botocore.session\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from botocore.auth import SigV4Auth\n",
    "from typing import Dict, List, Tuple\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "def get_inference(prompt: List) -> Tuple:\n",
    "    try:\n",
    "        modelId = 'anthropic.claude-v2:1'\n",
    "        accept = 'application/json'\n",
    "        contentType = 'application/json'\n",
    "        payload = json.dumps(prompt)\n",
    "        response = boto3_bedrock_runtime.invoke_model(body=payload, modelId=modelId, accept=accept, contentType=contentType)\n",
    "        response_body = json.loads(response.get('body').read()) \n",
    "        \n",
    "        status_code = response['ResponseMetadata']['HTTPStatusCode']\n",
    "        if status_code == 200:\n",
    "            print(response_body.get('completion'))\n",
    "            return (response_body.get('completion'))\n",
    "        else:\n",
    "            print(f\"Error: Received status code {status_code}, Response: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "async def async_calls_on_model(prompt):\n",
    "    return await asyncio.to_thread(get_inference, prompt)\n",
    "\n",
    "async def parallel_calls(prompts):\n",
    "    start_time = time.time()    \n",
    "    responses = await asyncio.gather(*[async_calls_on_model(prompt) for prompt in prompts])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"\\nAll tasks completed in {:.2f} seconds\".format(elapsed_time))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "with open(\"fake_data.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "       prompts.append(json.loads(line)['modelInput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "loop.create_task(parallel_calls(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch performance with 3000 samples with batch api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": \"<s3 uri of input data>\"\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": \"<s3 uri of output data>\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please visit the below webpage and follow the guidance to create a new role: BedrockBatchInferenceRole\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-permissions.html\n",
    "\n",
    "roleArn = \"arn:aws:iam::<account-id>:role/BedrockBatchInferenceRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=boto3_bedrock.create_model_invocation_job(\n",
    "    roleArn=roleArn,\n",
    "    modelId=modelId,\n",
    "    jobName=\"<jobname>\",\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobArn = response['jobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_bedrock.get_model_invocation_job(jobIdentifier=jobArn)['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_bedrock.get_model_invocation_job(jobIdentifier=jobArn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
